<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8" />
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="icon" href="/favicon.ico" type="image/x-icon" />


<!--css -->
<link rel="stylesheet"
	href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" />

<!--js -->
<script
	src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script
	src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
	
</script>
<style>
.footer {
	position: fixed;
	left: 0px;
	bottom: 0;
	width: 100%;
	height: 20px;
	text-align: center;
	line-height: 20px;
}

.center-block {
	padding: 100px;
}

.center-part {
	text-align: justified;
	padding: 20px;
}
</style>



<title>JeSemE</title>

</head>
<body>

	<div class="center-block">
		<h1>JeSemE Help</h1>
		<p>JeSemE is described in detail in our <a href="https://arxiv.org/abs/1807.04148">COLING 2018 paper "JeSemE: A Website for Exploring Diachronic Changes in Word Meaning and Emotion"</a> and our <a href="https://aclanthology.info/pdf/P/P17/P17-4006.pdf">ACL 2017 paper "Exploring Diachronic Lexical Semantics with JESEME"</a>, the
			following is a simplified overview.</p>
		<div class="center-part">
			<h2 id="metrics">Metrics</h2>
			<h3>
				VAD Emotions
			</h3>
			<p>
				The Valence Arousal Dominance (VAD) model of emotion assumes that affective states can be described relative to three emotional dimensions, 
				i.e., Valence (corresponding to the concept of polarity, see above), Arousal (the degree of excitement or calmness) and Dominance 
				(feeling in or out of control of a social situation). Emotion values for historical texts are calculated by combining word embeddings and 
				contemporary emotion lexicons, see <a href="https://arxiv.org/abs/1806.08115">"Inducing Affective Lexical Semantics in Historical Language"</a> for details. The following illustration shows the three-dimensional VAD space with the position of several emotion words.
			</p>
			<p><img src="vad-cube.png" alt="VAD emotions" height="350">	</p>
			<h3>
				PPMI and &chi;<sup>2</sup>
			</h3>
			<p>
				Both Positive Pointwise Mutual Information (PPMI) and Pearson's
				&chi;<sup>2</sup> measure how specific a combination of two words
				is. Both use the frequency of a word \(i\) or context word \(j\) to
				calculate the probability of finding one of them, as in \(P(i)\)
				respectively \(P(j)\).They then compare the expected probability of
				encountering both words \(P(i)P(j)\) with the observed
				frequency/probability \(P(i,j)\). Resulting values are normalized, so all associations for a word sum up to 1. JeSemE provides only values above 0.01, due to storage constraints.
			</p>
			<p>PPMI favors infrequent context words and can be calculated
				with: $$PPMI(i,j) := max(log\frac{P(i,j)}{P(i)P(j)},0)$$</p>
			<p>
				&chi;<sup>2</sup> is regarded as more balanced, we use a normalized version calculated
				with: $$\chi^2(i,j) := max(log\frac{(P(i,j) - P(i)P(j))^2}{P(i)P(j)},0)$$
			</p>

			<h3>
				SVD<sub>PPMI</sub>
			</h3>

			<p>
				SVD<sub>PPMI</sub> uses singular value decomposition (SVD) to reduce
				the dimensionality of a matrix storing PPMI data. It produces word
				embeddings with quality similar to word2vec embeddings. See <a
					href="http://www.aclweb.org/anthology/Q15-1016">Levy (2015)</a> for
				details. In contrast to word2vec, it is not affected by random
				initialization based realiability problems, see our papers <a
					href="http://aclweb.org/anthology/C/C16/C16-1262.pdf">"Bad Company&mdash;Neighborhoods in Neural Embedding Spaces Considered Harmful"</a> and <a
					href="https://dh2017.adho.org/abstracts/487/487.pdf">"Donâ€™t Get Fooled by Word Embeddings&mdash;Better Watch their Neighborhood"</a> for details.
			</p>
		</div>
		<div class="center-part">
			<h2 id="corpora">Corpora</h2>
			<h3>COHA</h3>
			<p>
				Corpus of Historical American English, representative and balanced.
				Lowercased during preprocessing. <a
					href="http://corpus.byu.edu/coha/">See here for more
					information.</a>
			</p>
			<h3>DTA</h3>
			<p>
				Deutsches Textarchiv 'German Text Archive', a representative (yet
				only vaguely balanced) corpus of ca. 1600-1900 German. Lemmatized
				during preprocessing. <a href="http://www.deutschestextarchiv.de">See
					here for more information in German.</a>
			</p>
			<h3>Google Books</h3>
			<p>
				The Google Books Ngram corpus covers about 6% of all books. We use
				the English Fiction and German subcorpus. It is unbalanced and known
				for sampling bias. English Fiction lowercased during preprocessing,
				German lemmatized and lowercased. <a
					href="https://books.google.com/ngrams">See here for Google's
					visualization.</a>
			</p>
			<h3>Royal Society Corpus</h3>
			<p>
				The Royal Society Corpus (RSC) contains the first two centuries
				of the Philosophical Transactions of the Royal Society of London.
				 The corpus was lemmatized and lowercased during preprocessing.
				<a
					href="https://fedora.clarin-d.uni-saarland.de/rsc/">See here
					for its homepage.</a>
			</p>
		</div>
		<div class="center-part">
			<h2 id="api">API</h2>
			JeSemE provides several APIs for GET requests returning JSON.
			All will preprocess words to match the queried corpus.
			<h3>Similar Words</h3>
			<p>Expects corpus, word1 and word2 as parameters, as in:</p>
			<p>
				<a
					href="http://JeSemE.org/api/similarity?word1=day&word2=night&corpus=coha">JeSemE.org/api/similarity?word1=day&amp;word2=night&amp;corpus=coha</a>
			</p>
			<h3>Word Emotion</h3>
			<p>Expects corpus and word as parameters, as in:</p>
			<p>
				<a
					href="http://jeseme.org/api/emotion?word=heart&corpus=coha">JeSemE.org/api/emotion?word=heart&amp;corpus=coha</a>
			</p>
			<h3>Typical Context</h3>
			<p>Expects corpus and word as parameters, Contexts can be
				requested for PPMI or &chi;<sup>2</sup> as in:</p>
			<p>
				<a
					href="http://jeseme.org/api/typicalcontextppmi?word=day&corpus=coha">JeSemE.org/api/typicalcontextppmi?word=day&amp;corpus=coha</a>
			</p>
			<p>
				<a
					href="http://jeseme.org/api/typicalcontextchi?word=day&corpus=coha">JeSemE.org/api/typicalcontextchi?word=day&amp;corpus=coha</a>
			</p>
			<h3>Relative Frequency</h3>
			<p>Expects corpus and word as parameters, as in:</p>
			<p>
				<a href="http://JeSemE.org/api/frequency?word=day&corpus=coha">JeSemE.org/api/frequency?word=day&amp;corpus=coha</a>
			</p>
		</div>
		<div class="center-part">
			<h2 id="download"> Download Models</h2>
			The models used by JeSemE are available for download. Each ZIP contains CSVs mapping words to IDs and IDs to decade-dependent statistics/word embeddings.
			<ul>
			  <li><a href="https://cloud.uni-jena.de/index.php/s/sZ4JcLCkFEz9NBg" download>COHA</a></li>
			  <li><a href="https://cloud.uni-jena.de/index.php/s/MzpPJBPMcscKYW5" download>DTA</a></li>
			  <li><a href="https://cloud.uni-jena.de/index.php/s/jGB6QJFX8Xc9xeW" download>Google Books Fiction</a></li>
			  <li><a href="https://cloud.uni-jena.de/index.php/s/WzcMDnY6G4ibF98" download>Google Books German</a></li>
			  <li><a href="https://cloud.uni-jena.de/index.php/s/DTa9D5TMz9e2DYC" download>RSC</a></li>
			</ul>
		</div> 
	</div>
	<footer class="footer">
		<p>
			<a href="help.html">Help</a>&emsp;&emsp;<a href="about.html">About</a>
		</p>
	</footer>
</body>


</html>
